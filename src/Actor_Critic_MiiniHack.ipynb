{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ryGCxd0b93wL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "import torch.optim as op\n",
        "import torch.nn.functional as torch_func\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import minihack\n",
        "from minihack import RewardManager\n",
        "from nle import nethack\n",
        "from gym import spaces\n",
        "import cv2\n",
        "cv2.ocl.setUseOpenCL(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yNhxm_D898WH"
      },
      "outputs": [],
      "source": [
        "# hyperparameters\n",
        "learning_rate = 5e-3\n",
        "\n",
        "# Constants\n",
        "GAMMA = 0.99\n",
        "\n",
        "PICTURE_HEIGHT = 21\n",
        "PICTURE_WIDTH = 79\n",
        "PICTURE_CHANNELS = 1\n",
        "\n",
        "\n",
        "ACTION_SPACE = [\n",
        "      nethack.CompassDirection.N,\n",
        "      nethack.CompassDirection.E,\n",
        "      nethack.CompassDirection.S,\n",
        "      nethack.CompassDirection.W,\n",
        "      nethack.CompassDirection.NE,\n",
        "      nethack.CompassDirection.SE,\n",
        "      nethack.CompassDirection.SW,\n",
        "      nethack.CompassDirection.NW,\n",
        "      nethack.CompassDirectionLonger.N,\n",
        "      nethack.CompassDirectionLonger.E,\n",
        "      nethack.CompassDirectionLonger.S,\n",
        "      nethack.CompassDirectionLonger.W,\n",
        "      nethack.CompassDirectionLonger.NE,\n",
        "      nethack.CompassDirectionLonger.SE,\n",
        "      nethack.CompassDirectionLonger.SW,\n",
        "      nethack.CompassDirectionLonger.NW,\n",
        "      nethack.Command.EAT,\n",
        "      nethack.MiscDirection.DOWN,\n",
        "      nethack.MiscDirection.WAIT,\n",
        "      nethack.MiscAction.MORE,\n",
        "      nethack.Command.ADJUST,\n",
        "      nethack.Command.APPLY,\n",
        "      nethack.Command.ATTRIBUTES,\n",
        "      nethack.Command.CALL,\n",
        "      nethack.Command.CAST,\n",
        "      nethack.Command.CHAT,\n",
        "      nethack.Command.CLOSE,\n",
        "      nethack.Command.DIP,\n",
        "      nethack.Command.DROP,\n",
        "      nethack.Command.DROPTYPE,\n",
        "      nethack.Command.ENGRAVE,\n",
        "      nethack.Command.ENHANCE,\n",
        "      nethack.Command.ESC,\n",
        "      nethack.Command.FIGHT,\n",
        "      nethack.Command.FIRE,\n",
        "      nethack.Command.FORCE,\n",
        "      nethack.Command.INVENTORY,\n",
        "      nethack.Command.INVENTTYPE,\n",
        "      nethack.Command.INVOKE,\n",
        "      nethack.Command.JUMP,\n",
        "      nethack.Command.KICK,\n",
        "      nethack.Command.LOOK,\n",
        "      nethack.Command.LOOT,\n",
        "      nethack.Command.MONSTER,\n",
        "      nethack.Command.MOVE,\n",
        "      nethack.Command.MOVEFAR,\n",
        "      nethack.Command.OFFER,\n",
        "      nethack.Command.OPEN,\n",
        "      nethack.Command.PAY,\n",
        "      nethack.Command.PICKUP,\n",
        "      nethack.Command.PRAY,\n",
        "      nethack.Command.PUTON,\n",
        "      nethack.Command.QUAFF,\n",
        "      nethack.Command.QUIVER,\n",
        "      nethack.Command.READ,\n",
        "      nethack.Command.REMOVE,\n",
        "      nethack.Command.RIDE,\n",
        "      nethack.Command.RUB,\n",
        "      nethack.Command.RUSH,\n",
        "      nethack.Command.RUSH2,\n",
        "      nethack.Command.SEARCH,\n",
        "      nethack.Command.SEEARMOR,\n",
        "      nethack.Command.SEERINGS,\n",
        "      nethack.Command.SEETOOLS,\n",
        "      nethack.Command.SEETRAP,\n",
        "      nethack.Command.SEEWEAPON,\n",
        "      nethack.Command.SHELL,\n",
        "      nethack.Command.SIT,\n",
        "      nethack.Command.SWAP,\n",
        "      nethack.Command.TAKEOFF,\n",
        "      nethack.Command.TAKEOFFALL,\n",
        "      nethack.Command.THROW,\n",
        "      nethack.Command.TIP,\n",
        "      nethack.Command.TURN,\n",
        "      nethack.Command.TWOWEAPON,\n",
        "      nethack.Command.UNTRAP,\n",
        "      nethack.Command.VERSIONSHORT,\n",
        "      nethack.Command.WEAR,\n",
        "      nethack.Command.WIELD,\n",
        "      nethack.Command.WIPE,\n",
        "      nethack.Command.ZAP,\n",
        "      nethack.TextCharacters.PLUS,\n",
        "      nethack.TextCharacters.QUOTE,\n",
        "      nethack.TextCharacters.DOLLAR,\n",
        "      nethack.TextCharacters.SPACE,\n",
        "]\n",
        "\n",
        "QUEST_ACTION_SPACE = [\n",
        "    nethack.CompassDirection.N,\n",
        "    nethack.CompassDirection.E,\n",
        "    nethack.CompassDirection.S,\n",
        "    nethack.CompassDirection.W,\n",
        "    nethack.Command.EAT,\n",
        "    nethack.Command.PICKUP,\n",
        "    nethack.Command.APPLY,\n",
        "    nethack.Command.FIRE,\n",
        "    nethack.Command.RUSH,\n",
        "    nethack.Command.ZAP,\n",
        "    nethack.Command.PUTON,\n",
        "    nethack.Command.READ,\n",
        "    nethack.Command.WEAR,\n",
        "    nethack.Command.QUAFF\n",
        "]\n",
        "\n",
        "MOVE_ACTION_SPACE = [\n",
        "      nethack.CompassDirection.N,\n",
        "      nethack.CompassDirection.E,\n",
        "      nethack.CompassDirection.S,\n",
        "      nethack.CompassDirection.W,\n",
        "      nethack.CompassDirection.NE,\n",
        "      nethack.CompassDirection.SE,\n",
        "      nethack.CompassDirection.SW,\n",
        "      nethack.CompassDirection.NW,\n",
        "      nethack.CompassDirectionLonger.N,\n",
        "      nethack.CompassDirectionLonger.E,\n",
        "      nethack.CompassDirectionLonger.S,\n",
        "      nethack.CompassDirectionLonger.W,\n",
        "      nethack.CompassDirectionLonger.NE,\n",
        "      nethack.CompassDirectionLonger.SE,\n",
        "      nethack.CompassDirectionLonger.SW,\n",
        "]\n",
        "\n",
        "\n",
        "EAT_ACTION_SPACE = [\n",
        "      nethack.CompassDirection.N,\n",
        "      nethack.CompassDirection.E,\n",
        "      nethack.CompassDirection.S,\n",
        "      nethack.CompassDirection.W,\n",
        "      nethack.CompassDirection.NE,\n",
        "      nethack.CompassDirection.SE,\n",
        "      nethack.CompassDirection.SW,\n",
        "      nethack.CompassDirection.NW,\n",
        "      nethack.CompassDirectionLonger.N,\n",
        "      nethack.CompassDirectionLonger.E,\n",
        "      nethack.CompassDirectionLonger.S,\n",
        "      nethack.CompassDirectionLonger.W,\n",
        "      nethack.CompassDirectionLonger.NE,\n",
        "      nethack.CompassDirectionLonger.SE,\n",
        "      nethack.CompassDirectionLonger.SW,\n",
        "      nethack.Command.EAT,\n",
        "]\n",
        "\n",
        "device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QMnDIEKZCS8J"
      },
      "outputs": [],
      "source": [
        "reward_manager = RewardManager()\n",
        "\n",
        "EAT_ITEMS = [\n",
        "    \"apple\",\n",
        "]\n",
        "\n",
        "MONSTERS = [\n",
        "    \"jackal\",\n",
        "    \"rat\",\n",
        "    \"lichen\"\n",
        "]\n",
        "\n",
        "WIELDED_ITEMS = [\n",
        "    \"dagger\",\n",
        "]\n",
        "\n",
        "for eat in EAT_ITEMS:\n",
        "  reward_manager.add_eat_event(name=eat, reward=1)\n",
        "\n",
        "for monster in MONSTERS:\n",
        "  reward_manager.add_kill_event(name=monster, reward=1)\n",
        "\n",
        "for wield in WIELDED_ITEMS:\n",
        "  reward_manager.add_wield_event(name=wield, reward=1)\n",
        "\n",
        "reward_manager.add_location_event(\"sink\", reward=-1, terminal_required=False)\n",
        "\n",
        "reward_manager.add_location_event(\"lava\", reward=-1, terminal_required=False)\n",
        "reward_manager.reward_lose = -1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lNRCvjwtUqD2"
      },
      "outputs": [],
      "source": [
        "\n",
        "envs = [\n",
        "    {\"name\": \"MiniHack-Room-Random-5x5-v0\", \"episodes\": 50, \"action_space\": MOVE_ACTION_SPACE, \"must_print\": False},\n",
        "    {\"name\": \"MiniHack-Eat-v0\", \"episodes\": 300, \"action_space\": EAT_ACTION_SPACE, \"must_print\": False},\n",
        "    {\"name\": \"MiniHack-LavaCross-Full-v0\", \"episodes\": 250, \"action_space\": QUEST_ACTION_SPACE, \"must_print\": False},\n",
        "]\n",
        "\n",
        "quest_envs = [\n",
        "    {\"name\": \"MiniHack-Quest-Easy-v0\", \"episodes\": 500, \"action_space\": QUEST_ACTION_SPACE, \"must_print\": False},\n",
        "    {\"name\": \"MiniHack-Quest-Medium-v0\", \"episodes\": 500, \"action_space\": QUEST_ACTION_SPACE, \"must_print\": False},\n",
        "    {\"name\": \"MiniHack-Quest-Hard-v0\", \"episodes\": 500, \"action_space\": QUEST_ACTION_SPACE, \"must_print\": False},\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l4nBPtmJ-Hsb"
      },
      "outputs": [],
      "source": [
        "class WarpFrame(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        \"\"\"Warp frames to 84x84 as done in the Nature paper and later work.\n",
        "        Expects inputs to be of shape height x width x num_channels\n",
        "        \"\"\"\n",
        "        gym.ObservationWrapper.__init__(self, env)\n",
        "        self.width = PICTURE_WIDTH\n",
        "        self.height = PICTURE_HEIGHT\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=0, high=255, shape=(self.height, self.width, 1), dtype=np.uint8\n",
        "        )\n",
        "\n",
        "    def observation(self, frame):\n",
        "        frame = frame[\"pixel\"]\n",
        "        # frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
        "        frame = cv2.resize(\n",
        "            frame, (self.width, self.height), interpolation=cv2.INTER_AREA\n",
        "        )\n",
        "        return frame[:, :, None]\n",
        "\n",
        "\n",
        "class GlyphWrapper(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        gym.ObservationWrapper.__init__(self, env)\n",
        "        self.LastFullImg = None\n",
        "\n",
        "    def observation(self, observation):\n",
        "        self.LastFullImg = observation[\"pixel\"]\n",
        "        glyphs = observation[\"glyphs\"]\n",
        "        glyphs_tensor = torch.tensor(glyphs, dtype=torch.float32)\n",
        "        flattened = glyphs_tensor.view(glyphs_tensor.size(0)*glyphs_tensor.size(1))\n",
        "        flattened = torch.div(flattened, other=torch.max(flattened))\n",
        "\n",
        "        return flattened.unsqueeze(0)\n",
        "\n",
        "class Glyp2DhWrapper(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        gym.ObservationWrapper.__init__(self, env)\n",
        "        self.LastFullImg = None\n",
        "\n",
        "    def observation(self, observation):\n",
        "        self.LastFullImg = observation[\"pixel\"]\n",
        "        glyphs = observation[\"glyphs\"]\n",
        "        glyphs_tensor = torch.tensor(glyphs, dtype=torch.float32)\n",
        "        glyphs_tensor = torch.div(glyphs_tensor, other=torch.max(glyphs_tensor))\n",
        "\n",
        "        glyphs_cropped = observation[\"glyphs_crop\"]\n",
        "        glyphs_cropped_tensor = torch.tensor(glyphs_cropped, dtype=torch.float32)\n",
        "        glyphs_cropped_tensor = torch.div(glyphs_cropped_tensor, other=torch.max(glyphs_cropped_tensor))\n",
        "\n",
        "        message = observation[\"message\"]\n",
        "        message_tensor = torch.tensor(message, dtype=torch.float32)\n",
        "        message_tensor = torch.div(message_tensor, other=torch.max(message_tensor)) if torch.max(message_tensor) != 0 else message_tensor\n",
        "\n",
        "        state ={\n",
        "            \"glyphs\": glyphs_tensor.unsqueeze(0).unsqueeze(0),\n",
        "            \"glyphs_cropped\": glyphs_cropped_tensor.unsqueeze(0).unsqueeze(0),\n",
        "            \"message\": message_tensor.unsqueeze(0)\n",
        "        }\n",
        "\n",
        "        return state\n",
        "\n",
        "    def get_last_full_img(self):\n",
        "        return self.LastFullImg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YCkdjqbA-Ihp"
      },
      "outputs": [],
      "source": [
        "class ActorCriticModel2D(torch.nn.Module):\n",
        "    def __init__(self, action_space):\n",
        "        super(ActorCriticModel2D, self).__init__()\n",
        "\n",
        "        self.action_space = action_space\n",
        "\n",
        "        # Activations\n",
        "        self.relu_layer = torch.nn.ReLU()\n",
        "        self.max_pooling = torch.nn.MaxPool2d(kernel_size=2)\n",
        "        self.tanh_layer = torch.nn.Tanh()\n",
        "\n",
        "        # Convolutional layers BIG\n",
        "        self.cnn_layer_one = torch.nn.Conv2d(in_channels=1, stride=1, kernel_size=(3, 3),\n",
        "                                             out_channels=8)\n",
        "        self.cnn_layer_two = torch.nn.Conv2d(in_channels=8, stride=1, kernel_size=(3, 3),\n",
        "                                             out_channels=16)\n",
        "\n",
        "        # Convolutional layers CROPPED\n",
        "        self.cnn_layer_one_crop = torch.nn.Conv2d(in_channels=1, stride=1, kernel_size=(3, 3),\n",
        "                                             out_channels=4)\n",
        "        self.cnn_layer_two_crop = torch.nn.Conv2d(in_channels=4, stride=1, kernel_size=(3, 3),\n",
        "                                             out_channels=8)\n",
        "\n",
        "        # Message layers\n",
        "        self.message_input = torch.nn.Linear(in_features=256, out_features=400)\n",
        "        self.message_hidden = torch.nn.Linear(in_features=400, out_features=100)\n",
        "\n",
        "        # Dummy input for flattened input\n",
        "        dummy_input = torch.zeros(1, PICTURE_CHANNELS, PICTURE_WIDTH, PICTURE_HEIGHT)\n",
        "        conv_out = self.forward_conv(dummy_input)\n",
        "        conv_out = conv_out.view(conv_out.size(0), -1)\n",
        "        dummy_message = torch.zeros((1, 256))\n",
        "        dummy_message = self.message_forward(dummy_message)\n",
        "        dummy_glyph_cropped = torch.zeros(1, PICTURE_CHANNELS, 9, 9)\n",
        "        cropped_conv_out = self.forward_conv_crop(dummy_glyph_cropped)\n",
        "        cropped_conv_out = cropped_conv_out.view(cropped_conv_out.size(0), -1)\n",
        "\n",
        "        dummy_output = torch.cat(tensors=(conv_out, cropped_conv_out, dummy_message), dim=1)\n",
        "        dummy_output_shape = int(np.prod(dummy_output.shape))\n",
        "\n",
        "        # Critic layers\n",
        "        self.critic_final = torch.nn.Linear(in_features=dummy_output_shape, out_features=1)\n",
        "\n",
        "        # Actor layers\n",
        "        self.actor_final = torch.nn.Linear(in_features=dummy_output_shape, out_features=self.action_space)\n",
        "\n",
        "    def forward_conv(self, state):\n",
        "        state = self.relu_layer(self.cnn_layer_one(state))\n",
        "        state = self.max_pooling(state)\n",
        "        state = self.relu_layer(self.cnn_layer_two(state))\n",
        "        state = self.max_pooling(state)\n",
        "        return state\n",
        "\n",
        "    def forward_conv_crop(self, state):\n",
        "        state = self.relu_layer(self.cnn_layer_one_crop(state))\n",
        "        state = self.max_pooling(state)\n",
        "        state = self.relu_layer(self.cnn_layer_two_crop(state))\n",
        "        return state\n",
        "\n",
        "    def actor_forward(self, state):\n",
        "        policy_dist = self.actor_final(state)\n",
        "        policy_softmax_dist = torch_func.softmax(policy_dist, dim=1)\n",
        "\n",
        "        return policy_softmax_dist\n",
        "\n",
        "    def critic_forward(self, state):\n",
        "        value = self.critic_final(state)\n",
        "\n",
        "        return value\n",
        "\n",
        "    def message_forward(self, state):\n",
        "        message_input = self.message_input(state)\n",
        "        message_input = self.tanh_layer(message_input)\n",
        "        message_hidden = self.message_hidden(message_input)\n",
        "        message_output = self.tanh_layer(message_hidden)\n",
        "\n",
        "        return message_output\n",
        "\n",
        "    def forward(self, state):\n",
        "        # Glyph\n",
        "        glyphs = state[\"glyphs\"]\n",
        "        conv_state = self.forward_conv(state=glyphs)\n",
        "        feature_extraction = conv_state.view(conv_state.size(0), -1)\n",
        "\n",
        "        # Cropped glyph\n",
        "        glyphs_cropped = state[\"glyphs_cropped\"]\n",
        "        cropped_conv_state = self.forward_conv_crop(state=glyphs_cropped)\n",
        "        cropped_feature_extraction = cropped_conv_state.view(cropped_conv_state.size(0), -1)\n",
        "\n",
        "        # Message state\n",
        "        message_state = self.message_forward(state[\"message\"])\n",
        "\n",
        "        output_state = torch.cat(tensors=(feature_extraction, message_state, cropped_feature_extraction), dim=1)\n",
        "\n",
        "        # Actor\n",
        "        policy_softmax_dist = self.actor_forward(output_state)\n",
        "\n",
        "        # Critic\n",
        "        value = self.critic_forward(output_state)\n",
        "\n",
        "        return policy_softmax_dist, value\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ftYgcnKz-QCV"
      },
      "outputs": [],
      "source": [
        "class ActorCriticRun():\n",
        "    def __init__(self, action_space, env_name, model, env, max_episodes, must_print=False):\n",
        "        self.env = env\n",
        "        self.A2C_model = model\n",
        "\n",
        "        self.all_lengths = []\n",
        "        self.average_lengths = []\n",
        "        self.all_rewards = []\n",
        "        self.entropy_term = 0\n",
        "        self.optimizer = op.Adam(self.A2C_model.parameters(), lr=learning_rate)\n",
        "        self.action_space = action_space\n",
        "        self.must_print = must_print\n",
        "        self.max_episodes = max_episodes\n",
        "        self.env_name = env_name\n",
        "\n",
        "    def run(self):\n",
        "\n",
        "        for ep in range(self.max_episodes):\n",
        "            log_probs = []\n",
        "            values = []\n",
        "            rewards = []\n",
        "\n",
        "            state = self.env.reset()\n",
        "            for step in range(num_steps):\n",
        "                policy_dist, value = self.A2C_model(state)\n",
        "                value = value.cpu().detach().numpy()[0][0]\n",
        "                dist = policy_dist.cpu().detach().numpy() #CHECK\n",
        "\n",
        "                action = np.random.choice(len(self.action_space), p=np.squeeze(dist)) #CHECK\n",
        "\n",
        "                log_prob = torch.log(policy_dist.squeeze(0)[action]) #CHECK\n",
        "                entropy = -np.sum(np.mean(dist)*np.log(dist)) #CHECK\n",
        "                next_state, reward, terminated, info = self.env.step(action)\n",
        "\n",
        "                rewards.append(reward)\n",
        "                values.append(value)\n",
        "                log_probs.append(log_prob)\n",
        "                self.entropy_term += entropy\n",
        "                state = next_state\n",
        "\n",
        "                if terminated or step == num_steps-1:\n",
        "                    _, Qval = self.A2C_model(next_state)\n",
        "                    Qval = torch.max(Qval.squeeze()) #CHECK\n",
        "                    self.all_rewards.append(np.sum(rewards))\n",
        "                    self.all_lengths.append(step+1)\n",
        "                    self.average_lengths.append(np.mean(self.all_lengths[-10:]))\n",
        "\n",
        "                    print(f\"episode: {ep+1}, reward: {np.sum(rewards)}, average_reward: {np.mean(self.all_rewards)}, total length: {step+1}, average length: {self.average_lengths[-1]}\")\n",
        "\n",
        "                    break\n",
        "\n",
        "            Qvals = np.zeros_like(values) #CHECK\n",
        "            for t in reversed(range(len(rewards))):\n",
        "                Qval = rewards[t] + GAMMA * Qval\n",
        "                Qvals[t] = Qval\n",
        "\n",
        "            values = torch.FloatTensor(values)\n",
        "            Qvals = torch.FloatTensor(Qvals)\n",
        "            log_probs = torch.stack(log_probs).cpu()\n",
        "\n",
        "            advantage = Qvals - values\n",
        "            actor_loss = (-log_probs*advantage).mean()  #CHECK\n",
        "            critic_loss = 0.5 * advantage.pow(2).mean()  #CHECK\n",
        "            ac_loss = actor_loss + critic_loss + 0.001 * self.entropy_term  #CHECK\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            ac_loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "\n",
        "    def plot_results(self):\n",
        "      plt.plot(self.all_rewards, label=\"All Rewards\")\n",
        "      plt.plot()\n",
        "      plt.xlabel(\"Episode\")\n",
        "      plt.ylabel(\"Reward\")\n",
        "      plt.title(self.env_name)\n",
        "      plt.legend()\n",
        "      plt.show()\n",
        "\n",
        "      plt.plot(self.all_lengths, label=\"All lengths\")\n",
        "      plt.plot(self.average_lengths, label=\"Avg lengths\")\n",
        "      plt.xlabel(\"Episode\")\n",
        "      plt.ylabel(\"Episode Length\")\n",
        "      plt.title(self.env_name)\n",
        "      plt.legend()\n",
        "      plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7erOdDsavUzg",
        "outputId": "eff5ce50-5862-4987-af5b-a92e600ac160"
      },
      "outputs": [],
      "source": [
        "agent_2d = ActorCriticModel2D(len(ACTION_SPACE))\n",
        "# agent_2d = agent_2d.cuda()\n",
        "\n",
        "for env_obj in envs:\n",
        "  print(f\"{'-'*5} Starting {env_obj['name']} {'-'*5} \")\n",
        "\n",
        "  env = gym.make(env_obj[\"name\"], actions=env_obj[\"action_space\"], observation_keys=(\"glyphs\", \"chars\", \"colors\", \"pixel\", \"message\", 'glyphs_crop'))\n",
        "  env = Glyp2DhWrapper(env)\n",
        "\n",
        "  a2c = ActorCriticRun(action_space=env_obj[\"action_space\"], env_name=env_obj[\"name\"], model=agent_2d, env=env, max_episodes=env_obj[\"episodes\"], must_print=env_obj[\"must_print\"])\n",
        "  a2c.run()\n",
        "  a2c.plot_results()\n",
        "\n",
        "  torch.save(agent_2d.state_dict(), \"/content/drive/MyDrive/University/Honours/RLWeights/a2c_2d_weights.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "BstpwCyjDGx5",
        "outputId": "332a021a-18af-4b35-d3bb-ca97328ccc88"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "for env_obj in quest_envs:\n",
        "  print(f\"{'-'*5} Starting {env_obj['name']} {'-'*5} \")\n",
        "  agent_quest = ActorCriticModel2D(len(QUEST_ACTION_SPACE))\n",
        "\n",
        "  env = gym.make(env_obj[\"name\"], actions=QUEST_ACTION_SPACE, observation_keys=(\"glyphs\", \"chars\", \"colors\", \"pixel\", \"message\", 'glyphs_crop'))\n",
        "  env = Glyp2DhWrapper(env)\n",
        "\n",
        "  a2c = ActorCriticRun(action_space=QUEST_ACTION_SPACE, env_name=env_obj[\"name\"], model=agent_quest, env=env, max_episodes=env_obj[\"episodes\"], must_print=env_obj[\"must_print\"])\n",
        "  a2c.run()\n",
        "  a2c.plot_results()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FTC6Awp-RFkY",
        "outputId": "b08d7cd6-6b0a-4381-a5f1-c91e24ed6c27"
      },
      "outputs": [],
      "source": [
        "videoFrames = []\n",
        "for env_obj in quest_envs:\n",
        "  print(f\"{'-'*5} Starting {env_obj['name']} {'-'*5} \")\n",
        "\n",
        "  env = gym.make(env_obj[\"name\"], actions=QUEST_ACTION_SPACE, observation_keys=(\"glyphs\", \"chars\", \"colors\", \"pixel\", \"message\", 'glyphs_crop'), reward_manager=reward_manager)\n",
        "  # self.env = WarpFrame(self.env)\n",
        "  env = Glyp2DhWrapper(env)\n",
        "\n",
        "  # agent_2d.set_action_space(env_obj[\"action_space\"])\n",
        "  a2c = ActorCriticRun(action_space=env_obj[\"action_space\"], env_name=env_obj[\"name\"], model=agent_quest, env=env, max_episodes=1, must_print=env_obj[\"must_print\"])\n",
        "  a2c.run(record=True)\n",
        "  videoFrames = a2c.get_frames()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kOhNcPZPROhu",
        "outputId": "68d814bd-565d-4f87-e388-6f4512a93dcd"
      },
      "outputs": [],
      "source": [
        "output_video_path = f'A2C_Quest_Hard.mp4'\n",
        "\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "fps = 5  # Frames per second\n",
        "frame_width, frame_height = videoFrames[0].shape[1], videoFrames[0].shape[0]\n",
        "\n",
        "out = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n",
        "\n",
        "for image in videoFrames:\n",
        "\n",
        "    frame = cv2.cvtColor(np.uint8(image), cv2.COLOR_RGB2BGR)\n",
        "    out.write(frame)\n",
        "out.release()\n",
        "\n",
        "print(f'Video saved at {output_video_path}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "7e1998ff7f8aa20ada591c520b972326324e5ea05489af9e422744c7c09f6dad"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
